{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "FK2e6N63Tt-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cd164d-e27c-4566-b959-b530234733cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keypoints saved to squat_keypoints.csv\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow_hub import load\n",
        "\n",
        "\n",
        "movenet = load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "def detect_keypoints(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_resized = cv2.resize(image, (192, 192))\n",
        "    image_resized = np.expand_dims(image_resized, axis=0).astype(np.int32)  # Convert to int32\n",
        "\n",
        "    outputs = movenet.signatures['serving_default'](tf.constant(image_resized, dtype=tf.int32))\n",
        "    keypoints = outputs['output_0'].numpy().reshape(-1, 3)  # 17 keypoints (x, y, confidence)\n",
        "    return keypoints\n",
        "\n",
        "data_folders = {\"train\": \"./train\", \"test\": \"./test\", \"valid\": \"./valid\"}\n",
        "output_csv = \"squat_keypoints.csv\"\n",
        "\n",
        "data = []\n",
        "for dataset, path in data_folders.items():\n",
        "    for label in [\"correct\", \"incorrect\"]:\n",
        "        label_path = os.path.join(path, label)\n",
        "        for img_name in os.listdir(label_path):\n",
        "            img_path = os.path.join(label_path, img_name)\n",
        "            keypoints = detect_keypoints(img_path)\n",
        "            row = [dataset, img_name]\n",
        "            for i in range(17):\n",
        "                row.extend([keypoints[i][0], keypoints[i][1], keypoints[i][2]])\n",
        "            row.append(label)\n",
        "            data.append(row)\n",
        "\n",
        "columns = [\"dataset\", \"image_name\"] + [f\"{coord}{i+1}\" for i in range(17) for coord in [\"x\", \"y\", \"c\"]] + [\"label\"]\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"Keypoints saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "data_path = \"squat_keypoints.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "edges = torch.tensor([\n",
        "    [0, 1], [0, 2], [1, 3], [2, 4], [0, 5], [0, 6], [5, 7],\n",
        "    [7, 9], [6, 8], [8, 10], [5, 6], [5, 11], [6, 12], [11, 13],\n",
        "    [13, 15], [12, 14], [14, 16], [11, 12]\n",
        "], dtype=torch.long).t()\n",
        "\n",
        "graphs = []\n",
        "for _, row in df.iterrows():\n",
        "    keypoints = [[row[f\"x{i+1}\"], row[f\"y{i+1}\"], row[f\"c{i+1}\"]] for i in range(17)]\n",
        "    x = torch.tensor(keypoints, dtype=torch.float)  # Node features\n",
        "    y = torch.tensor([1 if row['label'] == 'correct' else 0], dtype=torch.long)  # Label\n",
        "    graph = Data(x=x, edge_index=edges, y=y)\n",
        "    graphs.append(graph)\n",
        "\n",
        "print(f\"Processed {len(graphs)} graphs for GNN training.\")\n",
        "\n",
        "train_size = int(0.8 * len(graphs))\n",
        "train_graphs, test_graphs = graphs[:train_size], graphs[train_size:]\n",
        "\n",
        "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_graphs, batch_size=16, shuffle=False)\n",
        "\n",
        "class GNNClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 16)  # Input features: (x, y, confidence)\n",
        "        self.conv2 = GCNConv(16, 32)\n",
        "        self.fc = torch.nn.Linear(32, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)  # Pooling to get graph-level representation\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GNNClassifier().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Training the model...\")\n",
        "for epoch in range(500):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)  # Fix forward pass\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/500, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)  # Fix forward pass\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"gnn_model.pth\")\n",
        "print(\"Model saved as gnn_model.pth\")\n",
        "\n",
        "# Export the trained model to ONNX\n",
        "sample_graph = graphs[0].to(device)\n",
        "dummy_input = (sample_graph.x, sample_graph.edge_index, torch.zeros(sample_graph.x.shape[0], dtype=torch.long).to(device))\n",
        "\n",
        "torch.onnx.export(\n",
        "    model, dummy_input, \"gnn_model.onnx\",\n",
        "    export_params=True, opset_version=11, do_constant_folding=True,\n",
        "    input_names=[\"node_features\", \"edge_index\", \"batch\"],\n",
        "    output_names=[\"output\"]\n",
        ")\n",
        "\n",
        "print(\"Trained model saved in ONNX format as gnn_model.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW0BkSOxkOyl",
        "outputId": "21d20f7d-a918-4516-aff4-b3814349c370"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 159 graphs for GNN training.\n",
            "Training the model...\n",
            "Epoch 1/500, Loss: 0.6660\n",
            "Epoch 2/500, Loss: 0.6728\n",
            "Epoch 3/500, Loss: 0.6626\n",
            "Epoch 4/500, Loss: 0.6617\n",
            "Epoch 5/500, Loss: 0.6554\n",
            "Epoch 6/500, Loss: 0.6456\n",
            "Epoch 7/500, Loss: 0.6392\n",
            "Epoch 8/500, Loss: 0.6392\n",
            "Epoch 9/500, Loss: 0.6275\n",
            "Epoch 10/500, Loss: 0.6266\n",
            "Epoch 11/500, Loss: 0.6162\n",
            "Epoch 12/500, Loss: 0.6177\n",
            "Epoch 13/500, Loss: 0.5978\n",
            "Epoch 14/500, Loss: 0.6106\n",
            "Epoch 15/500, Loss: 0.6049\n",
            "Epoch 16/500, Loss: 0.6144\n",
            "Epoch 17/500, Loss: 0.5983\n",
            "Epoch 18/500, Loss: 0.6076\n",
            "Epoch 19/500, Loss: 0.5885\n",
            "Epoch 20/500, Loss: 0.6044\n",
            "Epoch 21/500, Loss: 0.6105\n",
            "Epoch 22/500, Loss: 0.5971\n",
            "Epoch 23/500, Loss: 0.5780\n",
            "Epoch 24/500, Loss: 0.5983\n",
            "Epoch 25/500, Loss: 0.6184\n",
            "Epoch 26/500, Loss: 0.5845\n",
            "Epoch 27/500, Loss: 0.5961\n",
            "Epoch 28/500, Loss: 0.5772\n",
            "Epoch 29/500, Loss: 0.5794\n",
            "Epoch 30/500, Loss: 0.5831\n",
            "Epoch 31/500, Loss: 0.5814\n",
            "Epoch 32/500, Loss: 0.5721\n",
            "Epoch 33/500, Loss: 0.5706\n",
            "Epoch 34/500, Loss: 0.5663\n",
            "Epoch 35/500, Loss: 0.5594\n",
            "Epoch 36/500, Loss: 0.5600\n",
            "Epoch 37/500, Loss: 0.5600\n",
            "Epoch 38/500, Loss: 0.5664\n",
            "Epoch 39/500, Loss: 0.5571\n",
            "Epoch 40/500, Loss: 0.5904\n",
            "Epoch 41/500, Loss: 0.5860\n",
            "Epoch 42/500, Loss: 0.5605\n",
            "Epoch 43/500, Loss: 0.5498\n",
            "Epoch 44/500, Loss: 0.5438\n",
            "Epoch 45/500, Loss: 0.5415\n",
            "Epoch 46/500, Loss: 0.5307\n",
            "Epoch 47/500, Loss: 0.5252\n",
            "Epoch 48/500, Loss: 0.5239\n",
            "Epoch 49/500, Loss: 0.5272\n",
            "Epoch 50/500, Loss: 0.5122\n",
            "Epoch 51/500, Loss: 0.5713\n",
            "Epoch 52/500, Loss: 0.5157\n",
            "Epoch 53/500, Loss: 0.5186\n",
            "Epoch 54/500, Loss: 0.5042\n",
            "Epoch 55/500, Loss: 0.4971\n",
            "Epoch 56/500, Loss: 0.4959\n",
            "Epoch 57/500, Loss: 0.5011\n",
            "Epoch 58/500, Loss: 0.5422\n",
            "Epoch 59/500, Loss: 0.5145\n",
            "Epoch 60/500, Loss: 0.4991\n",
            "Epoch 61/500, Loss: 0.4855\n",
            "Epoch 62/500, Loss: 0.5149\n",
            "Epoch 63/500, Loss: 0.5929\n",
            "Epoch 64/500, Loss: 0.5402\n",
            "Epoch 65/500, Loss: 0.5093\n",
            "Epoch 66/500, Loss: 0.5415\n",
            "Epoch 67/500, Loss: 0.5603\n",
            "Epoch 68/500, Loss: 0.5268\n",
            "Epoch 69/500, Loss: 0.4978\n",
            "Epoch 70/500, Loss: 0.4770\n",
            "Epoch 71/500, Loss: 0.4890\n",
            "Epoch 72/500, Loss: 0.4864\n",
            "Epoch 73/500, Loss: 0.4872\n",
            "Epoch 74/500, Loss: 0.4631\n",
            "Epoch 75/500, Loss: 0.4613\n",
            "Epoch 76/500, Loss: 0.4649\n",
            "Epoch 77/500, Loss: 0.4627\n",
            "Epoch 78/500, Loss: 0.4756\n",
            "Epoch 79/500, Loss: 0.4965\n",
            "Epoch 80/500, Loss: 0.4933\n",
            "Epoch 81/500, Loss: 0.4931\n",
            "Epoch 82/500, Loss: 0.4846\n",
            "Epoch 83/500, Loss: 0.4941\n",
            "Epoch 84/500, Loss: 0.4717\n",
            "Epoch 85/500, Loss: 0.4576\n",
            "Epoch 86/500, Loss: 0.4483\n",
            "Epoch 87/500, Loss: 0.4437\n",
            "Epoch 88/500, Loss: 0.4437\n",
            "Epoch 89/500, Loss: 0.4758\n",
            "Epoch 90/500, Loss: 0.5309\n",
            "Epoch 91/500, Loss: 0.4609\n",
            "Epoch 92/500, Loss: 0.4451\n",
            "Epoch 93/500, Loss: 0.4537\n",
            "Epoch 94/500, Loss: 0.4598\n",
            "Epoch 95/500, Loss: 0.4400\n",
            "Epoch 96/500, Loss: 0.4293\n",
            "Epoch 97/500, Loss: 0.4331\n",
            "Epoch 98/500, Loss: 0.4205\n",
            "Epoch 99/500, Loss: 0.4096\n",
            "Epoch 100/500, Loss: 0.4147\n",
            "Epoch 101/500, Loss: 0.4713\n",
            "Epoch 102/500, Loss: 0.4474\n",
            "Epoch 103/500, Loss: 0.4354\n",
            "Epoch 104/500, Loss: 0.4089\n",
            "Epoch 105/500, Loss: 0.4655\n",
            "Epoch 106/500, Loss: 0.4107\n",
            "Epoch 107/500, Loss: 0.4330\n",
            "Epoch 108/500, Loss: 0.4590\n",
            "Epoch 109/500, Loss: 0.4589\n",
            "Epoch 110/500, Loss: 0.4243\n",
            "Epoch 111/500, Loss: 0.4421\n",
            "Epoch 112/500, Loss: 0.4249\n",
            "Epoch 113/500, Loss: 0.4064\n",
            "Epoch 114/500, Loss: 0.4480\n",
            "Epoch 115/500, Loss: 0.4205\n",
            "Epoch 116/500, Loss: 0.4476\n",
            "Epoch 117/500, Loss: 0.4269\n",
            "Epoch 118/500, Loss: 0.4065\n",
            "Epoch 119/500, Loss: 0.4179\n",
            "Epoch 120/500, Loss: 0.3972\n",
            "Epoch 121/500, Loss: 0.4125\n",
            "Epoch 122/500, Loss: 0.4993\n",
            "Epoch 123/500, Loss: 0.5092\n",
            "Epoch 124/500, Loss: 0.4086\n",
            "Epoch 125/500, Loss: 0.3998\n",
            "Epoch 126/500, Loss: 0.4135\n",
            "Epoch 127/500, Loss: 0.3908\n",
            "Epoch 128/500, Loss: 0.3890\n",
            "Epoch 129/500, Loss: 0.4021\n",
            "Epoch 130/500, Loss: 0.4045\n",
            "Epoch 131/500, Loss: 0.3871\n",
            "Epoch 132/500, Loss: 0.3951\n",
            "Epoch 133/500, Loss: 0.3827\n",
            "Epoch 134/500, Loss: 0.4013\n",
            "Epoch 135/500, Loss: 0.3783\n",
            "Epoch 136/500, Loss: 0.4116\n",
            "Epoch 137/500, Loss: 0.3869\n",
            "Epoch 138/500, Loss: 0.3977\n",
            "Epoch 139/500, Loss: 0.3835\n",
            "Epoch 140/500, Loss: 0.4046\n",
            "Epoch 141/500, Loss: 0.4123\n",
            "Epoch 142/500, Loss: 0.4485\n",
            "Epoch 143/500, Loss: 0.3719\n",
            "Epoch 144/500, Loss: 0.4027\n",
            "Epoch 145/500, Loss: 0.3730\n",
            "Epoch 146/500, Loss: 0.3730\n",
            "Epoch 147/500, Loss: 0.3855\n",
            "Epoch 148/500, Loss: 0.3602\n",
            "Epoch 149/500, Loss: 0.3654\n",
            "Epoch 150/500, Loss: 0.4048\n",
            "Epoch 151/500, Loss: 0.4070\n",
            "Epoch 152/500, Loss: 0.3630\n",
            "Epoch 153/500, Loss: 0.3632\n",
            "Epoch 154/500, Loss: 0.3957\n",
            "Epoch 155/500, Loss: 0.3999\n",
            "Epoch 156/500, Loss: 0.4198\n",
            "Epoch 157/500, Loss: 0.3795\n",
            "Epoch 158/500, Loss: 0.3602\n",
            "Epoch 159/500, Loss: 0.3890\n",
            "Epoch 160/500, Loss: 0.4056\n",
            "Epoch 161/500, Loss: 0.3524\n",
            "Epoch 162/500, Loss: 0.3555\n",
            "Epoch 163/500, Loss: 0.3478\n",
            "Epoch 164/500, Loss: 0.3429\n",
            "Epoch 165/500, Loss: 0.3583\n",
            "Epoch 166/500, Loss: 0.3511\n",
            "Epoch 167/500, Loss: 0.3962\n",
            "Epoch 168/500, Loss: 0.3269\n",
            "Epoch 169/500, Loss: 0.3568\n",
            "Epoch 170/500, Loss: 0.3496\n",
            "Epoch 171/500, Loss: 0.3667\n",
            "Epoch 172/500, Loss: 0.3956\n",
            "Epoch 173/500, Loss: 0.3807\n",
            "Epoch 174/500, Loss: 0.3733\n",
            "Epoch 175/500, Loss: 0.3539\n",
            "Epoch 176/500, Loss: 0.3691\n",
            "Epoch 177/500, Loss: 0.3417\n",
            "Epoch 178/500, Loss: 0.3301\n",
            "Epoch 179/500, Loss: 0.3273\n",
            "Epoch 180/500, Loss: 0.3320\n",
            "Epoch 181/500, Loss: 0.3703\n",
            "Epoch 182/500, Loss: 0.3486\n",
            "Epoch 183/500, Loss: 0.3354\n",
            "Epoch 184/500, Loss: 0.3316\n",
            "Epoch 185/500, Loss: 0.3192\n",
            "Epoch 186/500, Loss: 0.3252\n",
            "Epoch 187/500, Loss: 0.3158\n",
            "Epoch 188/500, Loss: 0.3205\n",
            "Epoch 189/500, Loss: 0.3171\n",
            "Epoch 190/500, Loss: 0.3211\n",
            "Epoch 191/500, Loss: 0.3300\n",
            "Epoch 192/500, Loss: 0.4067\n",
            "Epoch 193/500, Loss: 0.4780\n",
            "Epoch 194/500, Loss: 0.3995\n",
            "Epoch 195/500, Loss: 0.3412\n",
            "Epoch 196/500, Loss: 0.3538\n",
            "Epoch 197/500, Loss: 0.3342\n",
            "Epoch 198/500, Loss: 0.3500\n",
            "Epoch 199/500, Loss: 0.3339\n",
            "Epoch 200/500, Loss: 0.3613\n",
            "Epoch 201/500, Loss: 0.3484\n",
            "Epoch 202/500, Loss: 0.3401\n",
            "Epoch 203/500, Loss: 0.3564\n",
            "Epoch 204/500, Loss: 0.3436\n",
            "Epoch 205/500, Loss: 0.3125\n",
            "Epoch 206/500, Loss: 0.3397\n",
            "Epoch 207/500, Loss: 0.3071\n",
            "Epoch 208/500, Loss: 0.3220\n",
            "Epoch 209/500, Loss: 0.3403\n",
            "Epoch 210/500, Loss: 0.3137\n",
            "Epoch 211/500, Loss: 0.3020\n",
            "Epoch 212/500, Loss: 0.2980\n",
            "Epoch 213/500, Loss: 0.3220\n",
            "Epoch 214/500, Loss: 0.3237\n",
            "Epoch 215/500, Loss: 0.3139\n",
            "Epoch 216/500, Loss: 0.3153\n",
            "Epoch 217/500, Loss: 0.3058\n",
            "Epoch 218/500, Loss: 0.3023\n",
            "Epoch 219/500, Loss: 0.3388\n",
            "Epoch 220/500, Loss: 0.2987\n",
            "Epoch 221/500, Loss: 0.3245\n",
            "Epoch 222/500, Loss: 0.3269\n",
            "Epoch 223/500, Loss: 0.3127\n",
            "Epoch 224/500, Loss: 0.2996\n",
            "Epoch 225/500, Loss: 0.3455\n",
            "Epoch 226/500, Loss: 0.3142\n",
            "Epoch 227/500, Loss: 0.3001\n",
            "Epoch 228/500, Loss: 0.2983\n",
            "Epoch 229/500, Loss: 0.3220\n",
            "Epoch 230/500, Loss: 0.3189\n",
            "Epoch 231/500, Loss: 0.3029\n",
            "Epoch 232/500, Loss: 0.2935\n",
            "Epoch 233/500, Loss: 0.3974\n",
            "Epoch 234/500, Loss: 0.3039\n",
            "Epoch 235/500, Loss: 0.3051\n",
            "Epoch 236/500, Loss: 0.3330\n",
            "Epoch 237/500, Loss: 0.3215\n",
            "Epoch 238/500, Loss: 0.3119\n",
            "Epoch 239/500, Loss: 0.2943\n",
            "Epoch 240/500, Loss: 0.2901\n",
            "Epoch 241/500, Loss: 0.2929\n",
            "Epoch 242/500, Loss: 0.2947\n",
            "Epoch 243/500, Loss: 0.3010\n",
            "Epoch 244/500, Loss: 0.2866\n",
            "Epoch 245/500, Loss: 0.3160\n",
            "Epoch 246/500, Loss: 0.3182\n",
            "Epoch 247/500, Loss: 0.3126\n",
            "Epoch 248/500, Loss: 0.2957\n",
            "Epoch 249/500, Loss: 0.2906\n",
            "Epoch 250/500, Loss: 0.2915\n",
            "Epoch 251/500, Loss: 0.2858\n",
            "Epoch 252/500, Loss: 0.3124\n",
            "Epoch 253/500, Loss: 0.3101\n",
            "Epoch 254/500, Loss: 0.2957\n",
            "Epoch 255/500, Loss: 0.3001\n",
            "Epoch 256/500, Loss: 0.2893\n",
            "Epoch 257/500, Loss: 0.2866\n",
            "Epoch 258/500, Loss: 0.2963\n",
            "Epoch 259/500, Loss: 0.2761\n",
            "Epoch 260/500, Loss: 0.4661\n",
            "Epoch 261/500, Loss: 0.4271\n",
            "Epoch 262/500, Loss: 0.3894\n",
            "Epoch 263/500, Loss: 0.3751\n",
            "Epoch 264/500, Loss: 0.3505\n",
            "Epoch 265/500, Loss: 0.3151\n",
            "Epoch 266/500, Loss: 0.3136\n",
            "Epoch 267/500, Loss: 0.3123\n",
            "Epoch 268/500, Loss: 0.2953\n",
            "Epoch 269/500, Loss: 0.2890\n",
            "Epoch 270/500, Loss: 0.2909\n",
            "Epoch 271/500, Loss: 0.2894\n",
            "Epoch 272/500, Loss: 0.2978\n",
            "Epoch 273/500, Loss: 0.3248\n",
            "Epoch 274/500, Loss: 0.3087\n",
            "Epoch 275/500, Loss: 0.2927\n",
            "Epoch 276/500, Loss: 0.2888\n",
            "Epoch 277/500, Loss: 0.3318\n",
            "Epoch 278/500, Loss: 0.2868\n",
            "Epoch 279/500, Loss: 0.3149\n",
            "Epoch 280/500, Loss: 0.3407\n",
            "Epoch 281/500, Loss: 0.3089\n",
            "Epoch 282/500, Loss: 0.3021\n",
            "Epoch 283/500, Loss: 0.2910\n",
            "Epoch 284/500, Loss: 0.3294\n",
            "Epoch 285/500, Loss: 0.3072\n",
            "Epoch 286/500, Loss: 0.3065\n",
            "Epoch 287/500, Loss: 0.2943\n",
            "Epoch 288/500, Loss: 0.2992\n",
            "Epoch 289/500, Loss: 0.2880\n",
            "Epoch 290/500, Loss: 0.2837\n",
            "Epoch 291/500, Loss: 0.3088\n",
            "Epoch 292/500, Loss: 0.3038\n",
            "Epoch 293/500, Loss: 0.2743\n",
            "Epoch 294/500, Loss: 0.2819\n",
            "Epoch 295/500, Loss: 0.2900\n",
            "Epoch 296/500, Loss: 0.2824\n",
            "Epoch 297/500, Loss: 0.3061\n",
            "Epoch 298/500, Loss: 0.3417\n",
            "Epoch 299/500, Loss: 0.3330\n",
            "Epoch 300/500, Loss: 0.3606\n",
            "Epoch 301/500, Loss: 0.3463\n",
            "Epoch 302/500, Loss: 0.2873\n",
            "Epoch 303/500, Loss: 0.2908\n",
            "Epoch 304/500, Loss: 0.2714\n",
            "Epoch 305/500, Loss: 0.2773\n",
            "Epoch 306/500, Loss: 0.2677\n",
            "Epoch 307/500, Loss: 0.2801\n",
            "Epoch 308/500, Loss: 0.3201\n",
            "Epoch 309/500, Loss: 0.3313\n",
            "Epoch 310/500, Loss: 0.3487\n",
            "Epoch 311/500, Loss: 0.2697\n",
            "Epoch 312/500, Loss: 0.2897\n",
            "Epoch 313/500, Loss: 0.2888\n",
            "Epoch 314/500, Loss: 0.2786\n",
            "Epoch 315/500, Loss: 0.2800\n",
            "Epoch 316/500, Loss: 0.2685\n",
            "Epoch 317/500, Loss: 0.2696\n",
            "Epoch 318/500, Loss: 0.2688\n",
            "Epoch 319/500, Loss: 0.2656\n",
            "Epoch 320/500, Loss: 0.2630\n",
            "Epoch 321/500, Loss: 0.2658\n",
            "Epoch 322/500, Loss: 0.2732\n",
            "Epoch 323/500, Loss: 0.2515\n",
            "Epoch 324/500, Loss: 0.2746\n",
            "Epoch 325/500, Loss: 0.2681\n",
            "Epoch 326/500, Loss: 0.2811\n",
            "Epoch 327/500, Loss: 0.2684\n",
            "Epoch 328/500, Loss: 0.3089\n",
            "Epoch 329/500, Loss: 0.2738\n",
            "Epoch 330/500, Loss: 0.2842\n",
            "Epoch 331/500, Loss: 0.2676\n",
            "Epoch 332/500, Loss: 0.2661\n",
            "Epoch 333/500, Loss: 0.2865\n",
            "Epoch 334/500, Loss: 0.3290\n",
            "Epoch 335/500, Loss: 0.3010\n",
            "Epoch 336/500, Loss: 0.2892\n",
            "Epoch 337/500, Loss: 0.2577\n",
            "Epoch 338/500, Loss: 0.2671\n",
            "Epoch 339/500, Loss: 0.2611\n",
            "Epoch 340/500, Loss: 0.2541\n",
            "Epoch 341/500, Loss: 0.2617\n",
            "Epoch 342/500, Loss: 0.2839\n",
            "Epoch 343/500, Loss: 0.2643\n",
            "Epoch 344/500, Loss: 0.3067\n",
            "Epoch 345/500, Loss: 0.2835\n",
            "Epoch 346/500, Loss: 0.2555\n",
            "Epoch 347/500, Loss: 0.2593\n",
            "Epoch 348/500, Loss: 0.3281\n",
            "Epoch 349/500, Loss: 0.2594\n",
            "Epoch 350/500, Loss: 0.2639\n",
            "Epoch 351/500, Loss: 0.3345\n",
            "Epoch 352/500, Loss: 0.2843\n",
            "Epoch 353/500, Loss: 0.2929\n",
            "Epoch 354/500, Loss: 0.2620\n",
            "Epoch 355/500, Loss: 0.3109\n",
            "Epoch 356/500, Loss: 0.2906\n",
            "Epoch 357/500, Loss: 0.3388\n",
            "Epoch 358/500, Loss: 0.2588\n",
            "Epoch 359/500, Loss: 0.2706\n",
            "Epoch 360/500, Loss: 0.2567\n",
            "Epoch 361/500, Loss: 0.2561\n",
            "Epoch 362/500, Loss: 0.2610\n",
            "Epoch 363/500, Loss: 0.2616\n",
            "Epoch 364/500, Loss: 0.2756\n",
            "Epoch 365/500, Loss: 0.2427\n",
            "Epoch 366/500, Loss: 0.2991\n",
            "Epoch 367/500, Loss: 0.2502\n",
            "Epoch 368/500, Loss: 0.2505\n",
            "Epoch 369/500, Loss: 0.2568\n",
            "Epoch 370/500, Loss: 0.2709\n",
            "Epoch 371/500, Loss: 0.2615\n",
            "Epoch 372/500, Loss: 0.2621\n",
            "Epoch 373/500, Loss: 0.2980\n",
            "Epoch 374/500, Loss: 0.2519\n",
            "Epoch 375/500, Loss: 0.2567\n",
            "Epoch 376/500, Loss: 0.2461\n",
            "Epoch 377/500, Loss: 0.2708\n",
            "Epoch 378/500, Loss: 0.2522\n",
            "Epoch 379/500, Loss: 0.2537\n",
            "Epoch 380/500, Loss: 0.2517\n",
            "Epoch 381/500, Loss: 0.2542\n",
            "Epoch 382/500, Loss: 0.2645\n",
            "Epoch 383/500, Loss: 0.3237\n",
            "Epoch 384/500, Loss: 0.2709\n",
            "Epoch 385/500, Loss: 0.3706\n",
            "Epoch 386/500, Loss: 0.3575\n",
            "Epoch 387/500, Loss: 0.3356\n",
            "Epoch 388/500, Loss: 0.4411\n",
            "Epoch 389/500, Loss: 0.4265\n",
            "Epoch 390/500, Loss: 0.4617\n",
            "Epoch 391/500, Loss: 0.3683\n",
            "Epoch 392/500, Loss: 0.3004\n",
            "Epoch 393/500, Loss: 0.2866\n",
            "Epoch 394/500, Loss: 0.2814\n",
            "Epoch 395/500, Loss: 0.2829\n",
            "Epoch 396/500, Loss: 0.2794\n",
            "Epoch 397/500, Loss: 0.3467\n",
            "Epoch 398/500, Loss: 0.3251\n",
            "Epoch 399/500, Loss: 0.2994\n",
            "Epoch 400/500, Loss: 0.3153\n",
            "Epoch 401/500, Loss: 0.2762\n",
            "Epoch 402/500, Loss: 0.2716\n",
            "Epoch 403/500, Loss: 0.2550\n",
            "Epoch 404/500, Loss: 0.2690\n",
            "Epoch 405/500, Loss: 0.2698\n",
            "Epoch 406/500, Loss: 0.2597\n",
            "Epoch 407/500, Loss: 0.2560\n",
            "Epoch 408/500, Loss: 0.3152\n",
            "Epoch 409/500, Loss: 0.2663\n",
            "Epoch 410/500, Loss: 0.2438\n",
            "Epoch 411/500, Loss: 0.2567\n",
            "Epoch 412/500, Loss: 0.2677\n",
            "Epoch 413/500, Loss: 0.2478\n",
            "Epoch 414/500, Loss: 0.2588\n",
            "Epoch 415/500, Loss: 0.2484\n",
            "Epoch 416/500, Loss: 0.2466\n",
            "Epoch 417/500, Loss: 0.2690\n",
            "Epoch 418/500, Loss: 0.2559\n",
            "Epoch 419/500, Loss: 0.2625\n",
            "Epoch 420/500, Loss: 0.2453\n",
            "Epoch 421/500, Loss: 0.2433\n",
            "Epoch 422/500, Loss: 0.2501\n",
            "Epoch 423/500, Loss: 0.2351\n",
            "Epoch 424/500, Loss: 0.2454\n",
            "Epoch 425/500, Loss: 0.2435\n",
            "Epoch 426/500, Loss: 0.2459\n",
            "Epoch 427/500, Loss: 0.2510\n",
            "Epoch 428/500, Loss: 0.2547\n",
            "Epoch 429/500, Loss: 0.2418\n",
            "Epoch 430/500, Loss: 0.2730\n",
            "Epoch 431/500, Loss: 0.2403\n",
            "Epoch 432/500, Loss: 0.2616\n",
            "Epoch 433/500, Loss: 0.2382\n",
            "Epoch 434/500, Loss: 0.2481\n",
            "Epoch 435/500, Loss: 0.3118\n",
            "Epoch 436/500, Loss: 0.2737\n",
            "Epoch 437/500, Loss: 0.2528\n",
            "Epoch 438/500, Loss: 0.2492\n",
            "Epoch 439/500, Loss: 0.2399\n",
            "Epoch 440/500, Loss: 0.2472\n",
            "Epoch 441/500, Loss: 0.2694\n",
            "Epoch 442/500, Loss: 0.2500\n",
            "Epoch 443/500, Loss: 0.2510\n",
            "Epoch 444/500, Loss: 0.2461\n",
            "Epoch 445/500, Loss: 0.2357\n",
            "Epoch 446/500, Loss: 0.2349\n",
            "Epoch 447/500, Loss: 0.2602\n",
            "Epoch 448/500, Loss: 0.2338\n",
            "Epoch 449/500, Loss: 0.2399\n",
            "Epoch 450/500, Loss: 0.2802\n",
            "Epoch 451/500, Loss: 0.2784\n",
            "Epoch 452/500, Loss: 0.2928\n",
            "Epoch 453/500, Loss: 0.2382\n",
            "Epoch 454/500, Loss: 0.2373\n",
            "Epoch 455/500, Loss: 0.2555\n",
            "Epoch 456/500, Loss: 0.2573\n",
            "Epoch 457/500, Loss: 0.2566\n",
            "Epoch 458/500, Loss: 0.2489\n",
            "Epoch 459/500, Loss: 0.2273\n",
            "Epoch 460/500, Loss: 0.2392\n",
            "Epoch 461/500, Loss: 0.2361\n",
            "Epoch 462/500, Loss: 0.2358\n",
            "Epoch 463/500, Loss: 0.2564\n",
            "Epoch 464/500, Loss: 0.2293\n",
            "Epoch 465/500, Loss: 0.2319\n",
            "Epoch 466/500, Loss: 0.2370\n",
            "Epoch 467/500, Loss: 0.2314\n",
            "Epoch 468/500, Loss: 0.2497\n",
            "Epoch 469/500, Loss: 0.2384\n",
            "Epoch 470/500, Loss: 0.2284\n",
            "Epoch 471/500, Loss: 0.2274\n",
            "Epoch 472/500, Loss: 0.2486\n",
            "Epoch 473/500, Loss: 0.2493\n",
            "Epoch 474/500, Loss: 0.2401\n",
            "Epoch 475/500, Loss: 0.2404\n",
            "Epoch 476/500, Loss: 0.2411\n",
            "Epoch 477/500, Loss: 0.2462\n",
            "Epoch 478/500, Loss: 0.2216\n",
            "Epoch 479/500, Loss: 0.2496\n",
            "Epoch 480/500, Loss: 0.2857\n",
            "Epoch 481/500, Loss: 0.2844\n",
            "Epoch 482/500, Loss: 0.2445\n",
            "Epoch 483/500, Loss: 0.2436\n",
            "Epoch 484/500, Loss: 0.2278\n",
            "Epoch 485/500, Loss: 0.2247\n",
            "Epoch 486/500, Loss: 0.2245\n",
            "Epoch 487/500, Loss: 0.2146\n",
            "Epoch 488/500, Loss: 0.2409\n",
            "Epoch 489/500, Loss: 0.2495\n",
            "Epoch 490/500, Loss: 0.2179\n",
            "Epoch 491/500, Loss: 0.2384\n",
            "Epoch 492/500, Loss: 0.2328\n",
            "Epoch 493/500, Loss: 0.2220\n",
            "Epoch 494/500, Loss: 0.2127\n",
            "Epoch 495/500, Loss: 0.2305\n",
            "Epoch 496/500, Loss: 0.2187\n",
            "Epoch 497/500, Loss: 0.2304\n",
            "Epoch 498/500, Loss: 0.2109\n",
            "Epoch 499/500, Loss: 0.2087\n",
            "Epoch 500/500, Loss: 0.2127\n",
            "Test Accuracy: 0.7500\n",
            "Model saved as gnn_model.pth\n",
            "Trained model saved in ONNX format as gnn_model.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:616: UserWarning: ONNX Preprocess - Removing mutation from node aten::scatter_add_ on block input: 'batch'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:349.)\n",
            "  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5333: UserWarning: Exporting aten::index operator with indices of type Byte. Only 1-D indices are supported. In any other case, this will produce an incorrect ONNX graph.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}